# Wearable_Pose_Model ✨✨


This is our research code of [Our_Model](https://user-images.githubusercontent.com/69295565/175981603-0d23d288-b1e6-4ebe-b9d9-5c9a82cc2733.png). 

Mesh Graphormer is a new transformer-based method for human pose and mesh reconsruction from an input image. In this work, we study how to combine graph convolutions and self-attentions in a transformer to better model both local and global interactions. 

 <img src="docs/graphormer_overview.png" width="650"> 

 <img src="docs/Fig1.gif" width="200"> <img src="docs/Fig2.gif" width="200"> <img src="docs/Fig3.gif" width="200">  <img src="docs/Fig4.gif" width="200"> 

 <img src="https://datarelease.blob.core.windows.net/metro/graphormer_demo.gif" width="200">

## Installation
Check [INSTALL.md](docs/INSTALL.md) for installation instructions.


## Model Zoo and Download
Please download our pre-trained models and other relevant files that are important to run our code. 

Check [DOWNLOAD.md](docs/DOWNLOAD.md) for details. 

## Quick demo
<img width="859" alt="스크린샷 2022-06-27 오후 11 24 13" src="">

We provide demo codes to run end-to-end inference on the test images.

Check [DEMO.md](docs/DEMO.md) for details.

## Experiments
We provide python codes for training and evaluation.

Check [EXP.md](docs/EXP.md) for details.
